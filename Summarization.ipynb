{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this Notebook, we explored the summarization of TED Talk transcripts with two main approaches: <ul>\n",
    "    <li> Keyword Weight (<a href='#keyword-weight'>Click to view</a>) </li>\n",
    "    <li> Text Rank(<a href='#textrank'>Click to view</a>) </li> </ul>\n",
    "  The end product of the both approaches are the key sentences in a transcript, which is an Extract summary of the transcript. <br>\n",
    "  Due to the knowledge and time constraint, we did not explore abstract summarization to obtain more \"short and sweet\" summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:25:04.049798Z",
     "start_time": "2020-08-02T10:24:47.331928Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import ast, pickle\n",
    "from functions.preprocessing import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:25:13.754467Z",
     "start_time": "2020-08-02T10:25:13.312468Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pickle.load(open('data/pickle/summary_df.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-work: the codes below do not have to be run except for cells containing functions##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T07:58:23.143779Z",
     "start_time": "2020-08-02T07:58:23.043707Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pickle.load(open('data/pickle/filtered_talks.p','rb'))\n",
    "df = df.reset_index(drop=True)\n",
    "df.url = df.url.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:26:16.972721Z",
     "start_time": "2020-08-02T10:26:16.964722Z"
    }
   },
   "outputs": [],
   "source": [
    "import contractions, re\n",
    "def sentence_tokenize(text):\n",
    "    fixed_contractions = contractions.fix(text)\n",
    "    removed_parenthesis = re.sub(\"[\\\\(].*?[\\\\)]\",\"\",string=fixed_contractions) \n",
    "    doc = nlp(removed_parenthesis)\n",
    "    return [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T08:22:25.658379Z",
     "start_time": "2020-08-02T07:58:25.821720Z"
    }
   },
   "outputs": [],
   "source": [
    "df['sents'] = df.transcript.map(sentence_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='keyword-weight'></a>\n",
    "<h1>Keyword Weight Approach</h1>\n",
    "\n",
    "> This approach utilizes the different types of keywords in a transcript to calculate a score for each sentence as a measurement of the importance to the transcript. The types of the keywords are <ul>\n",
    "    <li>Words in the title</li>\n",
    "    <li>Topic keywords in the top 3 likely topics of the transcript (30 in total) </li>\n",
    "    <li>Top 15 TF-IDF words in the transcript</li> </ul>\n",
    "  With regards to the way of scoring, we use one common formula: <b>Score of the sentence = Sum product of the unique keywords with their respective weights.</b> As for the weights of the keywords, we attempted three different sub-approaches:<ol>\n",
    "    <li>All keywords carry the weight of 1</li>\n",
    "    <li>The weight of a keyword = the number of its occurrence in the 3 types of keywords. E.g. if \"Apple\" appears in the title and topic keywords, it has a weight of 2.</li>\n",
    "    <li>The weight of a keyword = (base score + fix score if it appears in the title) * TF-IDF score * Topic keyword score (highest score if it appears in more than 1 topic in the top 3 topics) We use multiplication rather than addition is that the scores are all in decimal places and multiplication will make those less important words not stand out.</li></ol>\n",
    "    We did one last sub-approaches not on the keyword weights but to penalize the sentence by its length on top of the sub-approach 3.</li> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T08:34:35.175152Z",
     "start_time": "2020-08-02T08:34:34.349156Z"
    }
   },
   "outputs": [],
   "source": [
    "title_tokens = pickle.load(open('data/pickle/title_tokens.p','rb'))\n",
    "transcript_tokens = pickle.load(open('data/pickle/transcript_tokens.p','rb'))\n",
    "\n",
    "title_tokens = title_tokens.reset_index(drop=True)\n",
    "transcript_tokens =transcript_tokens.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T08:34:36.204709Z",
     "start_time": "2020-08-02T08:34:36.193710Z"
    }
   },
   "outputs": [],
   "source": [
    "df['title_tokens'] = title_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T08:34:39.522709Z",
     "start_time": "2020-08-02T08:34:37.624707Z"
    }
   },
   "outputs": [],
   "source": [
    "from functions.tfidf_vectorizer_functions import preprocessor, tokenizer\n",
    "vectorizer = pickle.load(open('model/tfidf_vectorizer.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T08:34:51.188729Z",
     "start_time": "2020-08-02T08:34:51.174749Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_tfidf_dict(idx):\n",
    "    if idx % 100 == 0:\n",
    "        print(idx, end=',')\n",
    "    transcript_str = \"{@}\".join(transcript_tokens[idx])\n",
    "    tfidf_matrix = vectorizer.transform([transcript_str])\n",
    "    feature_array = np.array(vectorizer.get_feature_names())\n",
    "    tfidf_idx_score =  sorted([(i,score) for i,score in enumerate(tfidf_matrix.toarray().flatten())],key=lambda x:x[1],reverse=True)\n",
    "    top_words_dict = {feature_array[i]:score for i,score in tfidf_idx_score[:15]}\n",
    "    \n",
    "    return top_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:32:00.823028Z",
     "start_time": "2020-08-02T08:34:52.155605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,"
     ]
    }
   ],
   "source": [
    "df['tfidf_dict'] = df.index.map(get_top_tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:43:57.857876Z",
     "start_time": "2020-08-02T09:43:52.617104Z"
    }
   },
   "outputs": [],
   "source": [
    "topicWordMatrix = pd.read_csv('data/output/topicWordMatrix.csv')\n",
    "docTop3TopicMatrix = pd.read_csv('data/output/docTop3TopicMatrix.csv')\n",
    "\n",
    "top3TopicKeywords = docTop3TopicMatrix.applymap(lambda x:list([ast.literal_eval(x) for x in topicWordMatrix.iloc[:,x]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:44:05.417646Z",
     "start_time": "2020-08-02T09:44:05.408594Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_topic_keywords_dict(idx):\n",
    "    topic_keywords_list = top3TopicKeywords.iloc[idx,:]\n",
    "    topic_keywords_dict = {}\n",
    "    for keyword_list in topic_keywords_list:\n",
    "        for (score,w) in keyword_list:\n",
    "            if w not in topic_keywords_dict or score > topic_keywords_dict[w]:\n",
    "                topic_keywords_dict[w] = score\n",
    "    return topic_keywords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:44:12.106702Z",
     "start_time": "2020-08-02T09:44:11.025703Z"
    }
   },
   "outputs": [],
   "source": [
    "df['topic_keywords_dict'] = df.index.map(lambda x: get_topic_keywords_dict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:44:48.281860Z",
     "start_time": "2020-08-02T09:44:48.270329Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2435, 2435, 2435, 2435)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(transcript_tokens), len(top3TopicKeywords), len(title_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:45:17.859290Z",
     "start_time": "2020-08-02T09:45:16.525746Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(df,open('data/pickle/summary_df.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-approach 1: All keywords carry the same weight of 1 ##\n",
    "\n",
    "> By looking at the result (top 15 sentences) of the first talk in the corpus, there are a few sentences that kind of capture the main ideas in the talk, such as sentence with indices 0, 4, 7, 8. They only cover the idea of education system issues in the talk at most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:25:23.717748Z",
     "start_time": "2020-08-02T10:25:23.706789Z"
    },
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "def sent_scoring1(idx): # present vs absent \n",
    "    row = df.iloc[idx,:]\n",
    "    sents_score_dict = {}\n",
    "    keywords = set(row['title_tokens']) | set(row['tfidf_dict'].keys()) | set(row['topic_keywords_dict'].keys())\n",
    "    for i, sent in enumerate(row['sents']):\n",
    "        sents_score_dict[i] = 0\n",
    "        for keyword in keywords:\n",
    "            if keyword in sent or keyword.lower() in sent or keyword.title() in sent:\n",
    "                sents_score_dict[i] +=1\n",
    "    sents_score_list = sorted(zip(sents_score_dict.keys(),sents_score_dict.values()), key=lambda x:x[1],reverse=True)           \n",
    "    \n",
    "    return sents_score_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:45:23.137823Z",
     "start_time": "2020-08-02T09:45:23.049825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from the first talk:\n",
      "0: And the consequence is that many highly-talented, brilliant, creative people think they are not, because the thing they were good at at school was not valued, or was actually stigmatized.\n",
      "1: There is not an education system on the planet that teaches dance everyday to children the way we teach them mathematics.\n",
      "2: No, she is good at some things, but if she is cooking, she is dealing with people on the phone, she is talking to the kids, she is painting the ceiling, she is doing open-heart surgery over here.\n",
      "3: If you think of it, children starting school this year will be retiring in 2065.\n",
      "4: And we are now running national education systems where mistakes are the worst thing you can make.\n",
      "5: what happens is, as children grow up, we start to educate them progressively from the waist up.\n",
      "6: if you look at the output, who really succeeds by this, who does everything that they should, who gets all the brownie points, who are the winners — I think you would have to conclude the whole purpose of public education throughout the world is to produce university professors.\n",
      "7: Our education system is predicated on the idea of academic ability.\n",
      "8: And the second is academic ability, which has really come to dominate our view of intelligence, because the universities designed the system in their image.\n",
      "9: If you think of it, the whole system of public education around the world is a protracted process of university entrance.\n",
      "10: More people, and it is the combination of all the things we have talked about — technology and its transformation effect on work, and demography and the huge explosion in population.\n",
      "11: Because she was disturbing people; her homework was always late; and so on, little kid of eight.\n",
      "12: If you are at a dinner party, and you say you work in education — Actually, you are not often at dinner parties, frankly.\n",
      "13: and you say you work in education\n",
      "14: I do not mean to say that being wrong is the same thing as being creative.\n"
     ]
    }
   ],
   "source": [
    "print(\"Result from the first talk:\")\n",
    "for i, s in enumerate([df.sents[0][x[0]] for x in sent_scoring1(0)[:15]]):\n",
    "    print(\"{}: {}\".format(i,s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-approach 2: Weight = No. of categories of keywords one belongs to ##\n",
    "\n",
    "> The result of sub-approach two is similar to that of sub-approach one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:25:26.508242Z",
     "start_time": "2020-08-02T10:25:26.492239Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent_scoring2(idx): # present vs absent \n",
    "    row = df.iloc[idx,:]\n",
    "    sents_score_dict = {}    \n",
    "    \n",
    "    keywords = set(row['title_tokens']) | set(row['tfidf_dict'].keys()) | set(row['topic_keywords_dict'].keys())\n",
    "    keyword_weight_dict = {}\n",
    "    for keyword in keywords:\n",
    "        keyword_weight_dict[keyword] = 0\n",
    "        if keyword in set(row['title_tokens']):\n",
    "            keyword_weight_dict[keyword] +=1 \n",
    "        if keyword in set(row['tfidf_dict'].keys()):\n",
    "            keyword_weight_dict[keyword] +=1 \n",
    "        if keyword in set(row['topic_keywords_dict'].keys()):\n",
    "            keyword_weight_dict[keyword] +=1 \n",
    "\n",
    "    for i, sent in enumerate(row['sents']):\n",
    "        sents_score_dict[i] = 0\n",
    "        for keyword in keywords:\n",
    "            if keyword in sent:\n",
    "                sents_score_dict[i] += keyword_weight_dict[keyword]\n",
    "            elif keyword.lower() in sent:\n",
    "                sents_score_dict[i] += keyword_weight_dict[keyword]\n",
    "            elif keyword.title() in sent:\n",
    "                sents_score_dict[i] += keyword_weight_dict[keyword]\n",
    "\n",
    "    sents_score_list = sorted(zip(sents_score_dict.keys(),sents_score_dict.values()), key=lambda x:x[1],reverse=True)           \n",
    "    \n",
    "    return sents_score_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T09:45:33.150118Z",
     "start_time": "2020-08-02T09:45:33.008599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from the first talk:\n",
      "0: And the consequence is that many highly-talented, brilliant, creative people think they are not, because the thing they were good at at school was not valued, or was actually stigmatized.\n",
      "1: If you think of it, children starting school this year will be retiring in 2065.\n",
      "2: There is not an education system on the planet that teaches dance everyday to children the way we teach them mathematics.\n",
      "3: So you were probably steered benignly away from things at school when you were a kid, things you liked, on the grounds that you would never get a job doing that.\n",
      "4: No, she is good at some things, but if she is cooking, she is dealing with people on the phone, she is talking to the kids, she is painting the ceiling, she is doing open-heart surgery over here.\n",
      "5: And we are now running national education systems where mistakes are the worst thing you can make.\n",
      "6: what happens is, as children grow up, we start to educate them progressively from the waist up.\n",
      "7: if you look at the output, who really succeeds by this, who does everything that they should, who gets all the brownie points, who are the winners — I think you would have to conclude the whole purpose of public education throughout the world is to produce university professors.\n",
      "8: Our education system is predicated on the idea of academic ability.\n",
      "9: If you think of it, the whole system of public education around the world is a protracted process of university entrance.\n",
      "10: And the school, in the '30s, wrote to her parents and said, \"We think Gillian has a learning disorder.\n",
      "11: So, this oak-paneled room, and she was there with her mother, and she was led and sat on this chair at the end, and she sat on her hands for 20 minutes while this man talked to her mother about the problems Gillian was having at school.\n",
      "12: Because she was disturbing people; her homework was always late; and so on, little kid of eight.\n",
      "13: If you are at a dinner party, and you say you work in education — Actually, you are not often at dinner parties, frankly.\n",
      "14: and you say you work in education\n"
     ]
    }
   ],
   "source": [
    "print(\"Result from the first talk:\")\n",
    "for i, s in enumerate([df.sents[0][x[0]] for x in sent_scoring2(0)[:15]]):\n",
    "    print(\"{}: {}\".format(i,s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-approach 3: Weight = (base score + score from title) * tfidf score * topic keyword score ##\n",
    "\n",
    "> The result of sub-approach is very different from the first two sub-approaches with many new sentences emerged. The quality of the top 15 sentences of this approach is higher than that of the previous two sub-approaches. Sentences at indices 0, 3, 4 and 7 do capture the idea about creativity, and those at 6, 8, 10 capture the same idea as the first two sub-approaches about the problem with our education system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:25:33.472727Z",
     "start_time": "2020-08-02T10:25:33.451726Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent_scoring3(idx): # present vs absent \n",
    "    row = df.iloc[idx,:]\n",
    "    sents_score_dict = {}    \n",
    "    \n",
    "    keywords = set(row['title_tokens']) | set(row['tfidf_dict'].keys()) | set(row['topic_keywords_dict'].keys())\n",
    "    keyword_weight_dict = {}\n",
    "    for keyword in keywords:\n",
    "        keyword_weight_dict[keyword] = 0.01 #base score\n",
    "        if keyword in set(row['title_tokens']):\n",
    "            keyword_weight_dict[keyword] += 0.02 # score from title\n",
    "        if keyword in set(row['tfidf_dict'].keys()):\n",
    "            keyword_weight_dict[keyword] *= row['tfidf_dict'][keyword]\n",
    "        if keyword in set(row['topic_keywords_dict'].keys()):\n",
    "            keyword_weight_dict[keyword] *= row['topic_keywords_dict'][keyword]\n",
    "\n",
    "    for i, sent in enumerate(row['sents']):\n",
    "        sents_score_dict[i] = 0\n",
    "        for keyword in keywords:\n",
    "            if keyword in sent:\n",
    "                sents_score_dict[i] += keyword_weight_dict[keyword]\n",
    "            elif keyword.lower() in sent:\n",
    "                sents_score_dict[i] += keyword_weight_dict[keyword]\n",
    "            elif keyword.title() in sent:\n",
    "                sents_score_dict[i] += keyword_weight_dict[keyword]\n",
    "\n",
    "    sents_score_list = sorted(zip(sents_score_dict.keys(),sents_score_dict.values()), key=lambda x:x[1],reverse=True)           \n",
    "    \n",
    "    return sents_score_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:25:41.055753Z",
     "start_time": "2020-08-02T10:25:40.956757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from the first talk:\n",
      "0: In fact, creativity — which I define as the process of having original ideas that have value — more often than not comes about through the interaction of different disciplinary ways of seeing things.\n",
      "1: One is the extraordinary evidence of human creativity in all of the presentations that we have had and in all of the people here.\n",
      "2: and I want to talk about creativity.\n",
      "3: My contention is that creativity now is as important in education as literacy, and we should treat it with the same status.\n",
      "4: I believe this passionately, that we do not grow into creativity, we grow out of it.\n",
      "5: And the second is academic ability, which has really come to dominate our view of intelligence, because the universities designed the system in their image.\n",
      "6: If you think of it, the whole system of public education around the world is a protracted process of university entrance.\n",
      "7: I do not mean to say that being wrong is the same thing as being creative.\n",
      "8: if you look at the output, who really succeeds by this, who does everything that they should, who gets all the brownie points, who are the winners — I think you would have to conclude the whole purpose of public education throughout the world is to produce university professors.\n",
      "9: I think now they would say she had ADHD.\n",
      "10: Our education system is predicated on the idea of academic ability.\n",
      "11: And the consequence is that many highly-talented, brilliant, creative people think they are not, because the thing they were good at at school was not valued, or was actually stigmatized.\n",
      "12: I saw a great t-shirt recently, which said, \"If a man speaks his mind in a forest, and no woman hears him, is he still wrong?\"And the third thing about intelligence is, it is distinct.\n",
      "13: If you were to visit education, as an alien, and say \"what is it for, public education?\n",
      "14: , we moved from Stratford to Los Angeles, and I just want to say a word about the transition.\n"
     ]
    }
   ],
   "source": [
    "print(\"Result from the first talk:\")\n",
    "for i, s in enumerate([df.sents[0][x[0]] for x in sent_scoring3(0)[:15]]):\n",
    "    print(\"{}: {}\".format(i,s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-approach 4: Normalize the score by the sentence length with a factor of 0.01 ##\n",
    "\n",
    "> Sub-approach 4 clearly penalized the length of the sentence to an extent that short sentences are prioritized too much. We also tried with the factor of 0.001 and the results do not seem to differ much. As such, the penalization does not work well in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:25:35.585407Z",
     "start_time": "2020-08-02T10:25:35.562406Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent_scoring4(idx): # present vs absent \n",
    "    row = df.iloc[idx,:]\n",
    "    sents_score_dict = {}    \n",
    "    \n",
    "    keywords = set(row['title_tokens']) | set(row['tfidf_dict'].keys()) | set(row['topic_keywords_dict'].keys())\n",
    "    keyword_weight_dict = {}\n",
    "    for keyword in keywords:\n",
    "        keyword_weight_dict[keyword] = 0.01 # base score\n",
    "        if keyword in set(row['title_tokens']):\n",
    "            keyword_weight_dict[keyword] += 0.02 # score from title\n",
    "        if keyword in set(row['tfidf_dict'].keys()):\n",
    "            keyword_weight_dict[keyword] *= row['tfidf_dict'][keyword]\n",
    "        if keyword in set(row['topic_keywords_dict'].keys()):\n",
    "            keyword_weight_dict[keyword] *= row['topic_keywords_dict'][keyword]\n",
    "\n",
    "    for i, sent in enumerate(row['sents']):\n",
    "        sents_score_dict[i] = 0\n",
    "        for keyword in keywords:\n",
    "            if keyword in sent:\n",
    "                sents_score_dict[i] += keyword_weight_dict[keyword]\n",
    "            elif keyword.lower() in sent:\n",
    "                sents_score_dict[i] += keyword_weight_dict[keyword]\n",
    "            elif keyword.title() in sent:\n",
    "                sents_score_dict[i] += keyword_weight_dict[keyword]\n",
    "        sents_score_dict[i] = sents_score_dict[i]/(len(sent) * 0.01)\n",
    "    sents_score_list = sorted(zip(sents_score_dict.keys(),sents_score_dict.values()), key=lambda x:x[1],reverse=True)           \n",
    "    \n",
    "    return sents_score_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:25:43.764276Z",
     "start_time": "2020-08-02T10:25:43.666274Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from the first talk:\n",
      "0: and I want to talk about creativity.\n",
      "1: I believe this passionately, that we do not grow into creativity, we grow out of it.\n",
      "2: My contention is that creativity now is as important in education as literacy, and we should treat it with the same status.\n",
      "3: One is the extraordinary evidence of human creativity in all of the presentations that we have had and in all of the people here.\n",
      "4: In fact, creativity — which I define as the process of having original ideas that have value — more often than not comes about through the interaction of different disciplinary ways of seeing things.\n",
      "5: I think now they would say she had ADHD.\n",
      "6: Was that wrong?\"\n",
      "7: and you say you work in education\n",
      "8: People who had to move to think.\n",
      "9: \" Who had to move to think.\n",
      "10: We were sitting there\n",
      "11: We need to radically rethink our view of intelligence.\n",
      "12: People who could not sit still.\n",
      "13: I do not mean to say that being wrong is the same thing as being creative.\n",
      "14: Our education system is predicated on the idea of academic ability.\n"
     ]
    }
   ],
   "source": [
    "print(\"Result from the first talk:\")\n",
    "for i, s in enumerate([df.sents[0][x[0]] for x in sent_scoring4(0)[:15]]):\n",
    "    print(\"{}: {}\".format(i,s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='textrank'></a>\n",
    "# Text Rank Approach #\n",
    "\n",
    "> This approach leverages the existing TextRank algorithm. It is a graph-based ranking algorithm where the importance of a sentence as the vertex in a graph is based on the sum of weighted “votes” cast by other sentences with the weights being the similarity between the two sentences. The algorithm is built on top of the PageRank algorithm on ranking sentences instead of documents. <br><br> Initially, we tried to apply the algorithm on all the sentences but failed due to the very large number of sentences each transcript contains. Thus, we conducted a filtering before applying the algorithm with the Keyword Weight sub-approach 3.<br><br> The results turned out to be not as good as the Keyword Weight sub-approach 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:25:47.069249Z",
     "start_time": "2020-08-02T10:25:46.539212Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations,product\n",
    "\n",
    "def build_similarity_matrix(sentences):\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    docs = [nlp(doc) for doc in sentences] \n",
    "    \n",
    "    for idx1,idx2 in combinations(range(len(docs)),r=2):\n",
    "        if idx1 != idx2:\n",
    "            score = docs[idx1].similarity(docs[idx2])\n",
    "            similarity_matrix[idx1][idx2] = score\n",
    "            similarity_matrix[idx2][idx1] = score\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:25:48.465724Z",
     "start_time": "2020-08-02T10:25:48.454726Z"
    }
   },
   "outputs": [],
   "source": [
    "def textRank_sents(sents):\n",
    "    sentence_similarity_martix = build_similarity_matrix(sents)\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph,max_iter=500)\n",
    "\n",
    "    ranked_sentence = sorted([(s,scores[i],) for i,s in enumerate(sents)], key=lambda x:x[1], reverse=True)     \n",
    "    return ranked_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-approach 1: Use all the sentences ##\n",
    "\n",
    "> When using all the sentences in a transcript, the large number of sentences may cause the pagerank algorithm in Networkx library to fail to converge even with 500 max iterations (the default is 100). As such, we decide to do a filtering of sentence before feeding them into the page rank algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T05:00:46.040792Z",
     "start_time": "2020-07-28T04:36:20.453964Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from the first talk:\n"
     ]
    },
    {
     "ename": "PowerIterationFailedConvergence",
     "evalue": "(PowerIterationFailedConvergence(...), 'power iteration failed to converge within 500 iterations')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPowerIterationFailedConvergence\u001b[0m           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-658c7cd1378d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Result from the first talk:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextRank_all_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-d85a57655a12>\u001b[0m in \u001b[0;36mtextRank_all_sents\u001b[1;34m(sents)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msentence_similarity_martix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_similarity_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msentence_similarity_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_similarity_martix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpagerank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_similarity_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mranked_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<D:\\Software\\Anaconda3\\lib\\site-packages\\decorator.py:decorator-gen-404>\u001b[0m in \u001b[0;36mpagerank\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda3\\lib\\site-packages\\networkx\\utils\\decorators.py\u001b[0m in \u001b[0;36m_not_implemented_for\u001b[1;34m(not_implement_for_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetworkXNotImplemented\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnot_implement_for_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_not_implemented_for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda3\\lib\\site-packages\\networkx\\algorithms\\link_analysis\\pagerank_alg.py\u001b[0m in \u001b[0;36mpagerank\u001b[1;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPowerIterationFailedConvergence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPowerIterationFailedConvergence\u001b[0m: (PowerIterationFailedConvergence(...), 'power iteration failed to converge within 500 iterations')"
     ]
    }
   ],
   "source": [
    "print(\"Result from the first talk:\")\n",
    "for i, s in enumerate(textRank_sents(df.sents[0])[:15]):\n",
    "    print(\"{}: {}\".format(i,s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-approach 2: Use the top 100 sentences selected using Keyword Weight - 3 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:25:58.954059Z",
     "start_time": "2020-08-02T10:25:58.924095Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_sents = [df.sents[0][x[0]] for x in sent_scoring3(0)][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:26:02.631813Z",
     "start_time": "2020-08-02T10:26:00.524812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from the first talk:\n",
      "0: ('if you look at the output, who really succeeds by this, who does everything that they should, who gets all the brownie points, who are the winners — I think you would have to conclude the whole purpose of public education throughout the world is to produce university professors.', 0.010423259616860902)\n",
      "1: ('there is something curious about professors in my experience — not all of them, but typically, they live in their heads.', 0.010373245902220336)\n",
      "2: ('And I like university professors, but you know, we should not hold them up as the high-water mark of all human achievement.', 0.010369232863169061)\n",
      "3: ('So you were probably steered benignly away from things at school when you were a kid, things you liked, on the grounds that you would never get a job doing that.', 0.0103587623833243)\n",
      "4: ('But they are rather curious, and I say this out of affection for them.', 0.01033855187805182)\n",
      "5: ('And she is exceptional, but I think she is not, so to speak, exceptional in the whole of childhood.', 0.010330253118634544)\n",
      "6: ('Because it is one of those things that goes deep with people, am I right?', 0.01032070444798935)\n",
      "7: ('And our task is to educate their whole being, so they can face this future.', 0.010280368155293004)\n",
      "8: ('Or rather, we get educated out of it.', 0.010280009935966606)\n",
      "9: ('So I have a big interest in education, and I think we all do.', 0.010278969854843466)\n",
      "10: ('And the consequence is that many highly-talented, brilliant, creative people think they are not, because the thing they were good at at school was not valued, or was actually stigmatized.', 0.010278871916896231)\n",
      "11: ('I do not mean to say that being wrong is the same thing as being creative.', 0.010271423350279725)\n",
      "12: ('And by the time they get to be adults, most kids have lost that capacity.', 0.010270985268288289)\n",
      "13: ('But now kids with degrees are often heading home to carry on playing video games, because you need an MA where the previous job required a BA, and now you need a PhD for the other.', 0.010268051068235947)\n",
      "14: ('No, she is good at some things, but if she is cooking, she is dealing with people on the phone, she is talking to the kids, she is painting the ceiling, she is doing open-heart surgery over here.', 0.010250598389623576)\n"
     ]
    }
   ],
   "source": [
    "print(\"Result from the first talk:\")\n",
    "for i, s in enumerate(textRank_sents(selected_sents)[:15]):\n",
    "    print(\"{}: {}\".format(i,s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of approaches and sub-approaches #\n",
    "\n",
    "> So far, the we use the first TED Talk, <i>Do schools kill creativity</i>, to measure the performance of the approaches and sub-approaches, and the measurement is based on human judgment rather than a standard quantitative measure. Moreover, we understand that different people may hold different opinions with respect to one summary. While it may be impossible to create a summary that satisfies everyone, we tried to create one that caters to as many people as possible. <br><br>\n",
    "    As such, we randomly selected 10 talks and four members in our team watched them and selected the top 15 sentences that they believed to be included in the summary individually. We also used a free online summarizer: autosummarizer.com. The summaries from each of the sub-approach of each approach were generated as well. Then we used 3 measures: Common sentence count, Cosine similarity and ROUGE-2 metrics to measure the performance.<br><br>\n",
    "    Due to some confusion, one member used the wrong talk url for her personal and autosummarizer evaluation. In the end, we will use 9 talks.<br><br>\n",
    "    From the results, we observed that top Cosine similarity values differs minimally. The common sentence count only show that on an average, our model-generated summary only matches at most one sentences with the human-generated summary. These two metrics are thus not so important in choosing the best approach. <br><br>\n",
    "Looking at the ROUGE2 metrics, there is a clear trade-off between Precision and Recall. The top 2 approaches in ROUGE2-F1 score are Key Weight sub-approach 2 and TextRank with Keyword Weight sub-approach 3. We believe that Recall is more important than Precision because lower Precision indicates more noise but lower Recall indicates missing out main points. Therefore, the final approach is TextRank with Keyword Weight sub-approach 3.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:26:04.449709Z",
     "start_time": "2020-08-02T10:26:04.443712Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_urls = ['https://www.ted.com/talks/gabby_giffords_and_mark_kelly_be_passionate_be_courageous_be_your_best',\n",
    " 'https://www.ted.com/talks/annmarie_thomas_squishy_circuits',\n",
    " 'https://www.ted.com/talks/bel_pesce_5_ways_to_kill_your_dreams',\n",
    " 'https://www.ted.com/talks/benjamin_barber_why_mayors_should_rule_the_world',\n",
    " 'https://www.ted.com/talks/nick_bostrom_on_our_biggest_problems',\n",
    " 'https://www.ted.com/talks/richard_dawkins_on_militant_atheism',\n",
    " 'https://www.ted.com/talks/elif_shafak_the_politics_of_fiction',\n",
    " 'https://www.ted.com/talks/roxane_gay_confessions_of_a_bad_feminist',\n",
    " 'https://www.ted.com/talks/mena_trott_tours_her_blog_world']\n",
    "\n",
    "#  'https://www.ted.com/talks/kristen_ashburn_s_heart_rending_pictures_of_aids' is ignored due to the confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:26:05.843649Z",
     "start_time": "2020-08-02T10:26:05.807628Z"
    }
   },
   "outputs": [],
   "source": [
    "xiaowei = pd.read_csv('data/input/summary/xiaowei.csv',encoding='iso-8859-1')\n",
    "may = pd.read_csv('data/input/summary/may.csv',encoding='iso-8859-1')\n",
    "suyee = pd.read_csv('data/input/summary/suyee.csv',encoding='iso-8859-1')\n",
    "autosummarizer =  pd.read_csv('data/input/summary/autosummarizer.csv',encoding='iso-8859-1')\n",
    "wende = pd.read_csv('data/input/summary/wende.csv',encoding='iso-8859-1')\n",
    "\n",
    "suyee.url = suyee.url.str.strip()\n",
    "xiaowei.url= xiaowei.url.str.strip()\n",
    "autosummarizer.url = autosummarizer.url.str.strip()\n",
    "may.url = may.url.str.strip()\n",
    "may = may.iloc[:,:2]\n",
    "wende.url = wende.url.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:26:32.830050Z",
     "start_time": "2020-08-02T10:26:24.852798Z"
    }
   },
   "outputs": [],
   "source": [
    "model_output = df[df.url.isin(eval_urls)][['transcript','url']]\n",
    "model_output['sents'] = model_output.transcript.apply(sentence_tokenize)\n",
    "model_output['kw_1'] =  model_output.index.map(sent_scoring1)\n",
    "model_output['kw_2'] =  model_output.index.map(sent_scoring2)\n",
    "model_output['kw_3'] =  model_output.index.map(sent_scoring3)\n",
    "model_output['kw_4'] =  model_output.index.map(sent_scoring4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:26:44.725639Z",
     "start_time": "2020-08-02T10:26:41.461434Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator_df = pd.DataFrame()\n",
    "evaluator_df['url'] = model_output.url\n",
    "\n",
    "evaluator_df = evaluator_df.merge(xiaowei,on='url')\n",
    "evaluator_df.rename({'summary':'xiaowei'},axis=1, inplace=True)\n",
    "\n",
    "evaluator_df = evaluator_df.merge(wende,on='url')\n",
    "evaluator_df.rename({'summary':'wende'},axis=1, inplace=True)\n",
    "\n",
    "evaluator_df = evaluator_df.merge(may,on='url')\n",
    "evaluator_df.rename({'summary':'may'},axis=1, inplace=True)\n",
    "\n",
    "evaluator_df = evaluator_df.merge(suyee,on='url')\n",
    "evaluator_df.rename({'summary':'suyee'},axis=1, inplace=True)\n",
    "\n",
    "evaluator_df = evaluator_df.merge(autosummarizer,on='url')\n",
    "evaluator_df.rename({'summary':'autosummarizer'},axis=1, inplace=True)\n",
    "\n",
    "evaluator_df.iloc[:,1:] = evaluator_df.iloc[:,1:].applymap(sentence_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:26:49.860659Z",
     "start_time": "2020-08-02T10:26:49.849625Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_sents(score_list, sents, num=10):\n",
    "    return [sents[idx] for (idx,score) in score_list[:num]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:27:16.729401Z",
     "start_time": "2020-08-02T10:26:55.356746Z"
    }
   },
   "outputs": [],
   "source": [
    "model_output['textrank-kw1'] = model_output.apply(lambda x:extract_sents(x['kw_1'],x['sents'],num=50),axis=1)\n",
    "model_output['textrank-kw1'] = model_output['textrank-kw1'].map(textRank_sents)\n",
    "model_output['textrank-kw1'] = model_output['textrank-kw1'].map(lambda x:[s[0] for s in x[:10]])\n",
    "\n",
    "model_output['textrank-kw2'] = model_output.apply(lambda x:extract_sents(x['kw_2'],x['sents'],num=50),axis=1)\n",
    "model_output['textrank-kw2'] = model_output['textrank-kw2'].map(textRank_sents)\n",
    "model_output['textrank-kw2'] = model_output['textrank-kw2'].map(lambda x:[s[0] for s in x[:10]])\n",
    "\n",
    "model_output['textrank-kw3'] = model_output.apply(lambda x:extract_sents(x['kw_3'],x['sents'],num=50),axis=1)\n",
    "model_output['textrank-kw3'] = model_output['textrank-kw3'].map(textRank_sents)\n",
    "model_output['textrank-kw3'] = model_output['textrank-kw3'].map(lambda x:[s[0] for s in x[:10]])\n",
    "\n",
    "model_output['textrank-kw4'] = model_output.apply(lambda x:extract_sents(x['kw_4'],x['sents'],num=50),axis=1)\n",
    "model_output['textrank-kw4'] = model_output['textrank-kw4'].map(textRank_sents)\n",
    "model_output['textrank-kw4'] = model_output['textrank-kw4'].map(lambda x:[s[0] for s in x[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:27:21.869442Z",
     "start_time": "2020-08-02T10:27:21.848440Z"
    }
   },
   "outputs": [],
   "source": [
    "model_output['kw_1'] = model_output.apply(lambda x:extract_sents(x['kw_1'],x['sents']),axis=1)\n",
    "model_output['kw_2'] = model_output.apply(lambda x:extract_sents(x['kw_2'],x['sents']),axis=1)\n",
    "model_output['kw_3'] = model_output.apply(lambda x:extract_sents(x['kw_3'],x['sents']),axis=1)\n",
    "model_output['kw_4'] = model_output.apply(lambda x:extract_sents(x['kw_4'],x['sents']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:27:23.494993Z",
     "start_time": "2020-08-02T10:27:23.483480Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(evaluator, approach, method):\n",
    "    eval_result = []\n",
    "    model_result = model_output[approach].tolist()\n",
    "    evaluator_result = evaluator_df[evaluator].tolist()\n",
    "    for idx in range(len(model_result)):\n",
    "        eval_result.append(method(model_result[idx],evaluator_result[idx]))\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:27:25.109667Z",
     "start_time": "2020-08-02T10:27:25.097663Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(model_output,open('data/pickle/summarizer_model_output.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T07:30:53.386716Z",
     "start_time": "2020-07-29T07:30:53.368196Z"
    }
   },
   "outputs": [],
   "source": [
    "model_output = pickle.load(open('data/pickle/summarizer_model_output.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Sentence Count ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:27:26.693415Z",
     "start_time": "2020-08-02T10:27:26.685413Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_same_sent(t1,t2):\n",
    "    sents1 = [t.strip() for t in t1]\n",
    "    sents2 = [t.strip() for t in t2]\n",
    "    return len(set(sents1) & set(sents2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:27:28.304609Z",
     "start_time": "2020-08-02T10:27:28.286608Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_sent_count_matrix = np.zeros((5,8,9)) # evaluator x approach x talk\n",
    "\n",
    "for i, e in enumerate(evaluator_df.columns[1:]):\n",
    "    for j, a in enumerate(model_output.columns[3:]):\n",
    "#         print(e,a, evaluate(e,a,count_same_sent))\n",
    "        common_sent_count_matrix[i][j] = evaluate(e,a,count_same_sent)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:27:29.973175Z",
     "start_time": "2020-08-02T10:27:29.939655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mean common sentences for the approaches: \n",
      "KW-1, KW-2, KW-3, KW-4, Textrank-kw1, Textrank-kw2, Textrank-kw3, Textrank-kw4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.68888889, 1.73333333, 1.6       , 1.31111111, 1.4       ,\n",
       "       1.46666667, 1.35555556, 1.55555556])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of mean common sentences for the approaches: \")\n",
    "print(\"KW-1, KW-2, KW-3, KW-4, Textrank-kw1, Textrank-kw2, Textrank-kw3, Textrank-kw4\")\n",
    "np.mean(common_sent_count_matrix,axis=(0,2)) # axis = (0,2) regardless of talk and evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:27:31.858690Z",
     "start_time": "2020-08-02T10:27:31.848692Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(t1,t2):\n",
    "    doc1 = nlp(' '.join(t1))\n",
    "    doc2 = nlp(' '.join(t2))\n",
    "    return doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:28:18.235853Z",
     "start_time": "2020-08-02T10:27:33.402386Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cosine_similarity_matrix = np.zeros((5,8,9)) # evaluator x approach x talk\n",
    "\n",
    "for i, e in enumerate(evaluator_df.columns[1:]):\n",
    "    for j, a in enumerate(model_output.columns[3:]):\n",
    "        cosine_similarity_matrix[i][j] = evaluate(e,a,cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:28:20.153361Z",
     "start_time": "2020-08-02T10:28:20.124358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cosine similarity for the approaches: \n",
      "KW-1, KW-2, KW-3, KW-4, Textrank-kw1, Textrank-kw2, Textrank-kw3, Textrank-kw4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99149228, 0.9905352 , 0.98603891, 0.90557505, 0.99107434,\n",
       "       0.99173125, 0.99171895, 0.91376191])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean cosine similarity for the approaches: \")\n",
    "print(\"KW-1, KW-2, KW-3, KW-4, Textrank-kw1, Textrank-kw2, Textrank-kw3, Textrank-kw4\")\n",
    "np.mean(cosine_similarity_matrix,axis=(0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-2 Metrics ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:28:22.052359Z",
     "start_time": "2020-08-02T10:28:22.034359Z"
    }
   },
   "outputs": [],
   "source": [
    "from rouge.metrics import rouge_n_summary_level\n",
    "\n",
    "def compute_rouge2(t1,t2):\n",
    "    doc1 = ' '.join(t1)\n",
    "    doc2 = ' '.join(t2)\n",
    "\n",
    "    score = rouge_n_summary_level(doc1,doc2,2)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:28:25.283224Z",
     "start_time": "2020-08-02T10:28:24.398614Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge2_matrix = np.zeros((5,8,9,3)) # evaluator x approach x talk x metrics (recall - precision - f1)\n",
    "\n",
    "for i, e in enumerate(evaluator_df.columns[1:]):\n",
    "    for j, a in enumerate(model_output.columns[3:]):\n",
    "        rouge2_matrix[i][j] = evaluate(e,a,compute_rouge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:28:27.251668Z",
     "start_time": "2020-08-02T10:28:27.229663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rouge-2 metric for the approaches: (Each row is an approach, the columns are Recall, Precision and F1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.80684015, 0.70085532, 0.7350154 ],\n",
       "       [0.78627783, 0.71936748, 0.73719144],\n",
       "       [0.65442494, 0.78583881, 0.69561985],\n",
       "       [0.32633929, 0.81840495, 0.44051703],\n",
       "       [0.81984058, 0.68168641, 0.72868632],\n",
       "       [0.8168782 , 0.69227431, 0.73490255],\n",
       "       [0.81090111, 0.70109097, 0.73838135],\n",
       "       [0.71448321, 0.66901297, 0.66165103]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean Rouge-2 metric for the approaches: (Each row is an approach, the columns are Recall, Precision and F1)\")\n",
    "rouge2_matrix.mean(axis=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:28:29.039249Z",
     "start_time": "2020-08-02T10:28:29.008248Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>common_sent_count</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>ROUGE-2-Recall</th>\n",
       "      <th>ROUGE-2-Precision</th>\n",
       "      <th>ROUGE-2-F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Keyword Weight-1</th>\n",
       "      <td>1.688889</td>\n",
       "      <td>0.991492</td>\n",
       "      <td>0.806840</td>\n",
       "      <td>0.700855</td>\n",
       "      <td>0.735015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keyword Weight-2</th>\n",
       "      <td>1.733333</td>\n",
       "      <td>0.990535</td>\n",
       "      <td>0.786278</td>\n",
       "      <td>0.719367</td>\n",
       "      <td>0.737191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keyword Weight-3</th>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.986039</td>\n",
       "      <td>0.654425</td>\n",
       "      <td>0.785839</td>\n",
       "      <td>0.695620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keyword Weight-4</th>\n",
       "      <td>1.311111</td>\n",
       "      <td>0.905575</td>\n",
       "      <td>0.326339</td>\n",
       "      <td>0.818405</td>\n",
       "      <td>0.440517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TextRank-kw1</th>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.991074</td>\n",
       "      <td>0.819841</td>\n",
       "      <td>0.681686</td>\n",
       "      <td>0.728686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TextRank-kw2</th>\n",
       "      <td>1.466667</td>\n",
       "      <td>0.991731</td>\n",
       "      <td>0.816878</td>\n",
       "      <td>0.692274</td>\n",
       "      <td>0.734903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TextRank-kw3</th>\n",
       "      <td>1.355556</td>\n",
       "      <td>0.991719</td>\n",
       "      <td>0.810901</td>\n",
       "      <td>0.701091</td>\n",
       "      <td>0.738381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TextRank-kw4</th>\n",
       "      <td>1.555556</td>\n",
       "      <td>0.913762</td>\n",
       "      <td>0.714483</td>\n",
       "      <td>0.669013</td>\n",
       "      <td>0.661651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  common_sent_count  cosine_similarity  ROUGE-2-Recall  \\\n",
       "Keyword Weight-1           1.688889           0.991492        0.806840   \n",
       "Keyword Weight-2           1.733333           0.990535        0.786278   \n",
       "Keyword Weight-3           1.600000           0.986039        0.654425   \n",
       "Keyword Weight-4           1.311111           0.905575        0.326339   \n",
       "TextRank-kw1               1.400000           0.991074        0.819841   \n",
       "TextRank-kw2               1.466667           0.991731        0.816878   \n",
       "TextRank-kw3               1.355556           0.991719        0.810901   \n",
       "TextRank-kw4               1.555556           0.913762        0.714483   \n",
       "\n",
       "                  ROUGE-2-Precision  ROUGE-2-F1  \n",
       "Keyword Weight-1           0.700855    0.735015  \n",
       "Keyword Weight-2           0.719367    0.737191  \n",
       "Keyword Weight-3           0.785839    0.695620  \n",
       "Keyword Weight-4           0.818405    0.440517  \n",
       "TextRank-kw1               0.681686    0.728686  \n",
       "TextRank-kw2               0.692274    0.734903  \n",
       "TextRank-kw3               0.701091    0.738381  \n",
       "TextRank-kw4               0.669013    0.661651  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = pd.DataFrame()\n",
    "eval_df['common_sent_count'] = np.mean(common_sent_count_matrix,axis=(0,2)).T\n",
    "eval_df['cosine_similarity'] = np.mean(cosine_similarity_matrix,axis=(0,2)).T\n",
    "eval_df['ROUGE-2-Recall'] = rouge2_matrix.mean(axis=(0,2)).T[0]\n",
    "eval_df['ROUGE-2-Precision'] = rouge2_matrix.mean(axis=(0,2)).T[1]\n",
    "eval_df['ROUGE-2-F1'] = rouge2_matrix.mean(axis=(0,2)).T[2]\n",
    "eval_df.index = ['Keyword Weight-1', 'Keyword Weight-2','Keyword Weight-3','Keyword Weight-4','TextRank-kw1','TextRank-kw2',\n",
    "                 'TextRank-kw3','TextRank-kw4']\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T10:32:17.203791Z",
     "start_time": "2020-08-02T10:32:16.963796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. if you look at the output, who really succeeds by this, who does everything that they should, who gets all the brownie points, who are the winners — I think you would have to conclude the whole purpose of public education throughout the world is to produce university professors.\n",
      "2. In fact, creativity — which I define as the process of having original ideas that have value — more often than not comes about through the interaction of different disciplinary ways of seeing things.\n",
      "3. My contention is that creativity now is as important in education as literacy, and we should treat it with the same status.\n",
      "4. I do not mean to say that being wrong is the same thing as being creative.\n",
      "5. I believe this passionately, that we do not grow into creativity, we grow out of it.\n",
      "6. One is the extraordinary evidence of human creativity in all of the presentations that we have had and in all of the people here.\n",
      "7. And the second is academic ability, which has really come to dominate our view of intelligence, because the universities designed the system in their image.\n",
      "8. If you think of it, the whole system of public education around the world is a protracted process of university entrance.\n",
      "9. and I want to talk about creativity.\n",
      "10. I think now they would say she had ADHD.\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "transcript_sents = df.sents[idx]\n",
    "scores = sent_scoring3(idx)\n",
    "top100 = extract_sents(scores,transcript_sents)\n",
    "ranked_sents = textRank_sents(top100)\n",
    "for i, s in enumerate([x[0] for x in ranked_sents[:10]]):\n",
    "    print(\"{}. {}\".format(i+1,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "219px",
    "width": "378px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
