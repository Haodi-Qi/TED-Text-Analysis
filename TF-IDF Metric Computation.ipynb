{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF metric computation ##\n",
    ">This steps is the pre-requisite step for Related Talk Recommendation.<br>\n",
    "    We use the preprocessed transcript tokens to fit the vectorizer. However, during the preprocessing, the entity that may consists of multiple words is considered as one token. If we were to use the built-in tokenizer and preprocessor in the Tfidfvectorizer, the entity will be split into indivual words and the meaning is lost. <br>\n",
    "    At the same time, the entity may not just be the examples the speaker use to illustrate his/her point of views and thus is not to the main point of the speech. We set a threshold of Term Frequency to filter them out.<br>\n",
    "    As such, a rather complicated string separator is specified with the customized tokenizer and preprocessor to avoid that issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T08:05:29.266471Z",
     "start_time": "2020-07-27T08:05:25.198988Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "from functions.preprocessing import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T08:05:29.736481Z",
     "start_time": "2020-07-27T08:05:29.269460Z"
    }
   },
   "outputs": [],
   "source": [
    "transcript_tokens= pickle.load(open('data/pickle/transcript_tokens.p','rb'))\n",
    "title_tokens= pickle.load(open('data/pickle/title_tokens.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T12:08:51.176347Z",
     "start_time": "2020-07-27T12:08:36.228783Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter tokens\n",
    "filtered_transcript_tokens = []\n",
    "for i, doc in enumerate(transcript_tokens):\n",
    "    to_remove = set()\n",
    "    for word in set(doc):\n",
    "        tf = doc.count(word)\n",
    "        if tf <= 5:  # threshold for tf\n",
    "            to_remove.add(word)\n",
    "    filtered_transcript_tokens.append([x for x in doc if x not in to_remove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T12:09:28.622935Z",
     "start_time": "2020-07-27T12:09:28.612932Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer(string):\n",
    "    global string_seperator\n",
    "    return string.split(string_seperator)\n",
    "\n",
    "def preprocessor(string):\n",
    "    return string\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3),tokenizer=tokenizer,preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T12:09:33.902431Z",
     "start_time": "2020-07-27T12:09:29.413913Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 3), norm='l2',\n",
       "                preprocessor=<function preprocessor at 0x000001F50FFBA730>,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function tokenizer at 0x000001F521907AE8>,\n",
       "                use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_seperator = \"{@}\"\n",
    "\n",
    "transcript_tokens_str = [string_seperator.join(tokens) for tokens in filtered_transcript_tokens]\n",
    "\n",
    "vectorizer.fit(transcript_tokens_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T12:10:30.452623Z",
     "start_time": "2020-07-27T12:10:25.491816Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open('model/tfidf_vectorizer.p','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
